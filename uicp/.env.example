# UICP Configuration
# Copy this file to .env and configure for your environment

# Cloud mode toggle (1 = direct cloud, 0 = local offload daemon)
USE_DIRECT_CLOUD=1

# Provider Router Configuration
# Enable the new provider router for unified backend routing
VITE_PROVIDER_ROUTER_V1=false

# Enable canary mode for specific providers (OpenAI, OpenRouter)
VITE_PROVIDER_ROUTER_CANARY=true

# Protocol Feature Flags
VITE_CHAT_PROTOCOL_V1=true
VITE_STREAM_V1=true
VITE_PROBLEM_DETAIL_V1=true
VITE_APPLY_HANDSHAKE_V1=true

# Ollama API key (required when USE_DIRECT_CLOUD=1)
OLLAMA_API_KEY=

# Additional Provider API Keys (set via UI or environment)
OPENAI_API_KEY=
OPENROUTER_API_KEY=
ANTHROPIC_API_KEY=

# Model selection now lives in the app under Agent Settings.
# No .env model variables are required.

# Ollama endpoints
OLLAMA_CLOUD_HOST=https://ollama.com
OLLAMA_LOCAL_BASE=http://127.0.0.1:11434/v1

# Security Configuration
UICP_POLICY=open
# UICP_SAFE_MODE=1  # Uncomment for emergency lockdown

# Development Configuration
VITE_WIL_ONLY=false  # Set to true for WIL-only mode (disables JSON/tool calling)
